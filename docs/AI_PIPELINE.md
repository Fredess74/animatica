# AI Pipeline

## Two-Phase Strategy

### Phase A: Manual Copy-Paste (MVP)

No API key required. No backend. Pure client-side.

```
User writes idea â†’ Clicks "Copy AI Prompt" â†’ Pastes into ChatGPT/Claude
â†’ Gets JSON back â†’ Pastes into Script Console â†’ Validate â†’ Build Scene
```

**Implementation:**

- `aiPromptTemplate.ts` contains a static prompt string with:
  - Full JSON schema reference
  - All actor types, properties, animation states
  - Camera angle suggestions
  - Style presets (noir, comedy, horror, anime, etc.)
  - `[USER_IDEA_HERE]` placeholder
- `ScriptConsole.tsx` has:
  - "ğŸ“‹ Copy AI Prompt" button â†’ copies prompt to clipboard
  - Textarea for pasting JSON result
  - "Validate" â†’ runs Zod schema
  - "Build Scene" â†’ calls `importScript()` â†’ renders in 3D

### Phase B: Direct API Integration (v2)

Backend handles LLM calls. User never leaves the editor.

```
User types idea â†’ Backend calls LLM â†’ Streams JSON â†’ Validates â†’ Auto-renders
```

**Implementation:**

- `POST /api/generate-scene` (Next.js API route)
  - Request: `{ prompt: string, style?: string, duration?: number }`
  - Response: Streaming JSON
  - Backend stores API keys (never exposed to client)
- Frontend shows live streaming progress
- Auto-validates on complete
- One-click "Build Scene" after preview

---

## AI Capabilities (Current + Future)

### MVP (Phase A)

| Capability | Status | How |
|-----------|--------|-----|
| Scene layout generation | âœ… | LLM generates actor positions, cameras, lights |
| Character setup | âœ… | Animation states, clothing, expressions |
| Keyframe animation | âœ… | Walk paths, camera moves, expression changes |
| Environment | âœ… | Weather, fog, sky color, lighting mood |
| Camera work | âœ… | Cut sequence, tracking shots, close-ups |

### v2 (Phase B)

| Capability | Status | How |
|-----------|--------|-----|
| Style selection | ğŸ”œ | User picks from genre buttons â†’ adjusts prompt context |
| Dialogue generation | ğŸ”œ | LLM writes character dialogue + TTS timing |
| AI scene editing | ğŸ”œ | "Make the camera closer" â†’ LLM patches existing JSON |
| Sound design | ğŸ”œ | Auto-suggest music mood, SFX placement |
| Automatic pacing | ğŸ”œ | LLM analyzes scene beats and adjusts timing |

### v3 (Future)

| Capability | Status | How |
|-----------|--------|-----|
| Voice generation | ğŸ”® | ElevenLabs / OpenAI TTS per character |
| Lip sync | ğŸ”® | Viseme mapping from audio â†’ morph targets |
| Music generation | ğŸ”® | Suno/Udio API for original soundtrack |
| SFX generation | ğŸ”® | AI-generated sound effects |
| Full film from text | ğŸ”® | Multi-scene, multi-act generation pipeline |

---

## Style Selection System

Users choose style via clickable preset buttons (not typing):

| Style | Lighting | Weather | Camera | Mood Color |
|-------|---------|---------|--------|-----------|
| ğŸ¬ **Noir** | Low key, harsh shadows | Rain | Low angles, Dutch tilts | `#1a1a2e` |
| ğŸ˜‚ **Comedy** | Bright, flat | Sunny | Wide shots, snappy cuts | `#fff4e6` |
| ğŸ‘» **Horror** | Dim, flickering | Fog/dust | POV, slow zoom | `#0a0a0a` |
| ğŸŒ¸ **Anime** | Cel-shaded, bloom | Cherry blossoms | Dynamic, speed lines | `#ffb7c5` |
| ğŸ¤  **Western** | Golden hour | Dust | Wide establishing, close-up eyes | `#c2956b` |
| ğŸš€ **Sci-Fi** | Neon, volumetric | None | Dolly, crane | `#00d4ff` |
| ğŸ’• **Romance** | Soft, warm | Light snow | Two-shots, shallow DOF | `#ff6b9d` |
| âš”ï¸ **Action** | High contrast | Varies | Handheld, quick cuts | `#ff4444` |

These presets inject additional context into the LLM prompt, guiding lighting, weather, camera style, and color palette.

---

## AI Prompt Template Structure

```
SYSTEM: You are a professional film director creating scenes for the Animatica animation engine.

SCHEMA: [full JSON schema here]

ACTOR TYPES:
- character: humanoid with animation, clothing, facial expressions
- primitive: box, sphere, cylinder, etc.
- light: point, spot, directional
- camera: perspective camera with fov/near/far

ANIMATION STATES: idle, walk, run, wave, talk, dance, sit, jump

STYLE: [selected style preset context]

RULES:
1. Always include at least one camera
2. Timeline duration between 10-60 seconds
3. Use cinematic camera movements
4. Add environmental storytelling (lighting, weather, fog)
5. Vary shot types (wide, medium, close-up)

USER IDEA: [placeholder]

OUTPUT: Valid JSON matching the ProjectSchema. Nothing else.
```

---

## Synergy Between Manual and Integrated Modes

Both modes use the **same JSON schema and validation pipeline**. This means:

1. Scenes created manually can be refined via AI
2. AI-generated scenes can be hand-edited in the editor
3. Export/import is always compatible
4. The AI prompt template evolves as the schema evolves
5. Users can switch between modes at any point

This decoupled architecture means the AI layer is a **skin on top of the engine**, not baked into it.
